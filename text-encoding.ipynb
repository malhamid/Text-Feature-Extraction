{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will be using two libraries for prepocessing the text: Keras and NLTK. Please note NLTK and Spacy are two libraries specialized in natural language processing. We will use them to remove stop words, stemming the tokens along with others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras library \n",
    "from keras.preprocessing.text import text_to_word_sequence as keras_text_to_word_sequence\n",
    "\n",
    "# Importing nltk library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as nltk_work_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Importing some other supporting libraries \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the next we will use for this small encoding experiment \n",
    "corpus = [\n",
    "    \"The number of trees in the world, according to a 2015 estimate, is 3.04 trillion.\",\n",
    "    \"46% of the trees in the world are in the tropics or sub-tropics.\",\n",
    "    \"20% of the trees in the world are in the temperate zones.\",\n",
    "    \"24% of threes in the world are in the coniferous boreal forests.\",\n",
    "    \"There about 15 billion trees are cut down annually.\",\n",
    "    \"There about 5 billion trees are planted annually.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The text preprocessing has five steps:\n",
    "Step 1. Tokenize the datasets seperating words in each sentence. \n",
    "Step 2. Remove the stop words.\n",
    "Step 3. Stemming the tokens, bringing each word to its root and in lower letters. \n",
    "Step 4. Remove punctuations, special characters, and words which have length equal to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 1. Tokenize the datasets seperating words in each sentence. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'number', 'of', 'trees', 'in', 'the', 'world', 'according', 'to', 'a', '2015', 'estimate', 'is', '3', '04', 'trillion'], ['46', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'tropics', 'or', 'sub', 'tropics'], ['20', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'temperate', 'zones'], ['24', 'of', 'threes', 'in', 'the', 'world', 'are', 'in', 'the', 'coniferous', 'boreal', 'forests'], ['there', 'about', '15', 'billion', 'trees', 'are', 'cut', 'down', 'annually'], ['there', 'about', '5', 'billion', 'trees', 'are', 'planted', 'annually']]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the tokens using Keras\n",
    "ks_tokens = []\n",
    "for text in corpus: \n",
    "    ks_tokens.append(keras_text_to_word_sequence(text))\n",
    "print(ks_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'trees',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'according',\n",
       "  'to',\n",
       "  'a',\n",
       "  '2015',\n",
       "  'estimate',\n",
       "  'is',\n",
       "  '3',\n",
       "  '04',\n",
       "  'trillion'],\n",
       " ['46',\n",
       "  'of',\n",
       "  'the',\n",
       "  'trees',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'tropics',\n",
       "  'or',\n",
       "  'sub',\n",
       "  'tropics'],\n",
       " ['20',\n",
       "  'of',\n",
       "  'the',\n",
       "  'trees',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'temperate',\n",
       "  'zones'],\n",
       " ['24',\n",
       "  'of',\n",
       "  'threes',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'coniferous',\n",
       "  'boreal',\n",
       "  'forests'],\n",
       " ['there',\n",
       "  'about',\n",
       "  '15',\n",
       "  'billion',\n",
       "  'trees',\n",
       "  'are',\n",
       "  'cut',\n",
       "  'down',\n",
       "  'annually'],\n",
       " ['there', 'about', '5', 'billion', 'trees', 'are', 'planted', 'annually']]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'number', 'of', 'trees', 'in', 'the', 'world', ',', 'according', 'to', 'a', '2015', 'estimate', ',', 'is', '3.04', 'trillion', '.'], ['46', '%', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'tropics', 'or', 'sub-tropics', '.'], ['20', '%', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'temperate', 'zones', '.'], ['24', '%', 'of', 'threes', 'in', 'the', 'world', 'are', 'in', 'the', 'coniferous', 'boreal', 'forests', '.'], ['There', 'about', '15', 'billion', 'trees', 'are', 'cut', 'down', 'annually', '.'], ['There', 'about', '5', 'billion', 'trees', 'are', 'planted', 'annually', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the tokens using NLTK\n",
    "# We need to serialize the text to splits the works into tokens\n",
    "nltk_tokens = [] \n",
    "for text in corpus:\n",
    "    pre_text = pd.Series(text).str.cat(sep=' ')\n",
    "    nltk_tokens.append(word_tokenize(pre_text))\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 2. Remove the stop words. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'other', 'of', 'him', 'didn', 'have', 'haven', 'or', 'won', 'couldn']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriving the stop words from the library \n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Examples of some stop words\n",
    "list(stop_words)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['number',\n",
       "  'trees',\n",
       "  'world',\n",
       "  'according',\n",
       "  '2015',\n",
       "  'estimate',\n",
       "  '3',\n",
       "  '04',\n",
       "  'trillion'],\n",
       " ['46', 'trees', 'world', 'tropics', 'sub', 'tropics'],\n",
       " ['20', 'trees', 'world', 'temperate', 'zones'],\n",
       " ['24', 'threes', 'world', 'coniferous', 'boreal', 'forests'],\n",
       " ['15', 'billion', 'trees', 'cut', 'annually'],\n",
       " ['5', 'billion', 'trees', 'planted', 'annually']]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(ks_tokens)): \n",
    "    ks_tokens[i] = [w for w in pd.Series(ks_tokens[i]) if not w in stop_words]\n",
    "ks_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['number',\n",
       "  'trees',\n",
       "  'world',\n",
       "  ',',\n",
       "  'according',\n",
       "  '2015',\n",
       "  'estimate',\n",
       "  ',',\n",
       "  '3.04',\n",
       "  'trillion',\n",
       "  '.'],\n",
       " ['46', '%', 'trees', 'world', 'tropics', 'sub-tropics', '.'],\n",
       " ['20', '%', 'trees', 'world', 'temperate', 'zones', '.'],\n",
       " ['24', '%', 'threes', 'world', 'coniferous', 'boreal', 'forests', '.'],\n",
       " ['15', 'billion', 'trees', 'cut', 'annually', '.'],\n",
       " ['5', 'billion', 'trees', 'planted', 'annually', '.']]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlike the list generated by Keras libary, here we have to make sure all tokens are either capitalized or in lower case.\n",
    "for i in range(0, len(nltk_tokens)): \n",
    "    nltk_tokens[i] = [tok.lower() for tok in nltk_tokens[i]]\n",
    "    nltk_tokens[i] = [w for w in nltk_tokens[i] if not w in stop_words]\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 3. Stemming the tokens, bringing each word to its root. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# We will use Porter algorithm to reduce the words. Porter has 5 phases of word reductions: \n",
    "porter = PorterStemmer()\n",
    "print(porter.stem('Eating'))\n",
    "print(porter.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['number', 'tree', 'world', 'accord', '2015', 'estim', '3', '04', 'trillion'], ['46', 'tree', 'world', 'tropic', 'sub', 'tropic'], ['20', 'tree', 'world', 'temper', 'zone'], ['24', 'three', 'world', 'conifer', 'boreal', 'forest'], ['15', 'billion', 'tree', 'cut', 'annual'], ['5', 'billion', 'tree', 'plant', 'annual']]\n"
     ]
    }
   ],
   "source": [
    "for tokens in ks_tokens:\n",
    "    for i in range(0, len(tokens)):    \n",
    "        tokens[i] = porter.stem(tokens[i])\n",
    "print(ks_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['number', 'tree', 'world', ',', 'accord', '2015', 'estim', ',', '3.04', 'trillion', '.'], ['46', '%', 'tree', 'world', 'tropic', 'sub-trop', '.'], ['20', '%', 'tree', 'world', 'temper', 'zone', '.'], ['24', '%', 'three', 'world', 'conifer', 'boreal', 'forest', '.'], ['15', 'billion', 'tree', 'cut', 'annual', '.'], ['5', 'billion', 'tree', 'plant', 'annual', '.']]\n"
     ]
    }
   ],
   "source": [
    "for tokens in nltk_tokens:\n",
    "    for i in range (0,len(tokens)):    \n",
    "        tokens[i] = porter.stem(tokens[i])\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 4. Remove punctuations, special characters, and words which have length equal to 1. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['number', 'tree', 'world', 'accord', 'estim', 'trillion'], ['tree', 'world', 'tropic', 'sub', 'tropic'], ['tree', 'world', 'temper', 'zone'], ['three', 'world', 'conifer', 'boreal', 'forest'], ['billion', 'tree', 'cut', 'annual'], ['billion', 'tree', 'plant', 'annual']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(ks_tokens)):\n",
    "    new_ks_tokens = []\n",
    "    for tok in ks_tokens[i]: \n",
    "        tok = tok.translate(str.maketrans('', '', string.punctuation))\n",
    "        if tok != \"\" and len(tok) > 1 and (tok.isnumeric()==False):\n",
    "            new_ks_tokens.append(tok)\n",
    "    ks_tokens[i] = new_ks_tokens\n",
    "print(ks_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number', 'tree', 'world', 'accord', 'estim', 'trillion']\n",
      "['tree', 'world', 'tropic', 'sub', 'tropic']\n",
      "['tree', 'world', 'temper', 'zone']\n",
      "['three', 'world', 'conifer', 'boreal', 'forest']\n",
      "['billion', 'tree', 'cut', 'annual']\n",
      "['billion', 'tree', 'plant', 'annual']\n"
     ]
    }
   ],
   "source": [
    "for sentences in ks_tokens: \n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The previous steps has many nexted loops and can easily be very expensive to compute if the number of setences in our corpus grow significantly. Hence, we will repeat the previous steps with few lines of code using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to get an instance of the tokenizer class\n",
    "tok = Tokenizer()\n",
    "# Keras can fit our corpus in a single call\n",
    "tok.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency of each word in the corpus:\n",
      "OrderedDict([('the', 10), ('number', 1), ('of', 4), ('trees', 5), ('in', 7), ('world', 4), ('according', 1), ('to', 1), ('a', 1), ('2015', 1), ('estimate', 1), ('is', 1), ('3', 1), ('04', 1), ('trillion', 1), ('46', 1), ('are', 5), ('tropics', 2), ('or', 1), ('sub', 1), ('20', 1), ('temperate', 1), ('zones', 1), ('24', 1), ('threes', 1), ('coniferous', 1), ('boreal', 1), ('forests', 1), ('there', 2), ('about', 2), ('15', 1), ('billion', 2), ('cut', 1), ('down', 1), ('annually', 2), ('5', 1), ('planted', 1)])\n"
     ]
    }
   ],
   "source": [
    "# Let's explore what is the content of the tokenized corpus\n",
    "print('The frequency of each word in the corpus:')\n",
    "print(tok.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences in the corpus:\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print('The number of sentences in the corpus:')\n",
    "print(tok.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The number of trees in the world, according to a 2015 estimate, is 3.04 trillion.',\n",
       " '46% of the trees in the world are in the tropics or sub-tropics.',\n",
       " '20% of the trees in the world are in the temperate zones.',\n",
       " '24% of threes in the world are in the coniferous boreal forests.',\n",
       " 'There about 15 billion trees are cut down annually.',\n",
       " 'There about 5 billion trees are planted annually.']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of each extracted tokens:\n",
      "{'the': 1, 'in': 2, 'trees': 3, 'are': 4, 'of': 5, 'world': 6, 'tropics': 7, 'there': 8, 'about': 9, 'billion': 10, 'annually': 11, 'number': 12, 'according': 13, 'to': 14, 'a': 15, '2015': 16, 'estimate': 17, 'is': 18, '3': 19, '04': 20, 'trillion': 21, '46': 22, 'or': 23, 'sub': 24, '20': 25, 'temperate': 26, 'zones': 27, '24': 28, 'threes': 29, 'coniferous': 30, 'boreal': 31, 'forests': 32, '15': 33, 'cut': 34, 'down': 35, '5': 36, 'planted': 37}\n"
     ]
    }
   ],
   "source": [
    "print('The index of each extracted tokens:')\n",
    "print(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The appearnce of each words in sentences:\n",
      "defaultdict(<class 'int'>, {'3': 1, 'estimate': 1, 'the': 4, 'of': 4, 'according': 1, 'a': 1, 'trillion': 1, 'world': 4, 'in': 4, 'to': 1, 'trees': 5, 'is': 1, 'number': 1, '2015': 1, '04': 1, '46': 1, 'or': 1, 'are': 5, 'sub': 1, 'tropics': 1, 'temperate': 1, '20': 1, 'zones': 1, 'coniferous': 1, 'boreal': 1, '24': 1, 'threes': 1, 'forests': 1, '15': 1, 'annually': 2, 'cut': 1, 'billion': 2, 'about': 2, 'down': 1, 'there': 2, 'planted': 1, '5': 1})\n"
     ]
    }
   ],
   "source": [
    "print('The appearnce of each words in sentences:')\n",
    "print(tok.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The number of trees in the world, according to a 2015 estimate, is 3.04 trillion.',\n",
       " '46% of the trees in the world are in the tropics or sub-tropics.',\n",
       " '20% of the trees in the world are in the temperate zones.',\n",
       " '24% of threes in the world are in the coniferous boreal forests.',\n",
       " 'There about 15 billion trees are cut down annually.',\n",
       " 'There about 5 billion trees are planted annually.']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'in': 2,\n",
       " 'trees': 3,\n",
       " 'are': 4,\n",
       " 'of': 5,\n",
       " 'world': 6,\n",
       " 'tropics': 7,\n",
       " 'there': 8,\n",
       " 'about': 9,\n",
       " 'billion': 10,\n",
       " 'annually': 11,\n",
       " 'number': 12,\n",
       " 'according': 13,\n",
       " 'to': 14,\n",
       " 'a': 15,\n",
       " '2015': 16,\n",
       " 'estimate': 17,\n",
       " 'is': 18,\n",
       " '3': 19,\n",
       " '04': 20,\n",
       " 'trillion': 21,\n",
       " '46': 22,\n",
       " 'or': 23,\n",
       " 'sub': 24,\n",
       " '20': 25,\n",
       " 'temperate': 26,\n",
       " 'zones': 27,\n",
       " '24': 28,\n",
       " 'threes': 29,\n",
       " 'coniferous': 30,\n",
       " 'boreal': 31,\n",
       " 'forests': 32,\n",
       " '15': 33,\n",
       " 'cut': 34,\n",
       " 'down': 35,\n",
       " '5': 36,\n",
       " 'planted': 37}"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> Approach #1: One Hot Encoding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tok.texts_to_matrix(corpus, mode='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 3. 2. 1. 1. 1. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 3. 2. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 2. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the frequency of each word in the sentence (TFIDF)\n",
    "print(tok.texts_to_matrix(corpus, mode='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.125      0.0625     0.0625     0.         0.0625\n",
      "  0.0625     0.         0.         0.         0.         0.\n",
      "  0.0625     0.0625     0.0625     0.0625     0.0625     0.0625\n",
      "  0.0625     0.0625     0.0625     0.0625     0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.21428571 0.14285714 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.14285714 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07142857 0.07142857\n",
      "  0.07142857 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.25       0.16666667 0.08333333 0.08333333 0.08333333\n",
      "  0.08333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08333333 0.08333333 0.08333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.16666667 0.16666667 0.         0.08333333 0.08333333\n",
      "  0.08333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08333333 0.08333333\n",
      "  0.08333333 0.08333333 0.08333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.11111111 0.11111111 0.\n",
      "  0.         0.         0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.125      0.125      0.\n",
      "  0.         0.         0.125      0.125      0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.125      0.125     ]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the frequency of each word in the sentence\n",
    "print(tok.texts_to_matrix(corpus, mode='freq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
