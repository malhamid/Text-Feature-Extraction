{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will be using two libraries for prepocessing the text: Keras and NLTK. Please note NLTK and Spacy are two libraries specialized in natural language processing. We will use them to remove stop words, stemming the tokens along with others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras library \n",
    "from keras.preprocessing.text import text_to_word_sequence as keras_text_to_word_sequence\n",
    "\n",
    "# Importing nltk library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as nltk_work_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Importing some other supporting libraries \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the next we will use for this small encoding experiment \n",
    "corpus = [\n",
    "    \"The number of trees in the world, according to a 2015 estimate, is 3.04 trillion.\",\n",
    "    \"46% of the trees in the world are in the tropics or sub-tropics.\",\n",
    "    \"20% of the trees in the world are in the temperate zones.\",\n",
    "    \"24% of threes in the world are in the coniferous boreal forests.\",\n",
    "    \"There about 15 billion trees are cut down annually.\",\n",
    "    \"There about 5 billion trees are planted annually.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The text preprocessing has five steps:\n",
    "Step 1. Tokenize the datasets seperating words in each sentence. \n",
    "Step 2. Remove the stop words.\n",
    "Step 3. Stemming the tokens, bringing each word to its root and in lower letters. \n",
    "Step 4. Remove punctuations, special characters, and words which have length equal to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 1. Tokenize the datasets seperating words in each sentence. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'number', 'of', 'trees', 'in', 'the', 'world', 'according', 'to', 'a', '2015', 'estimate', 'is', '3', '04', 'trillion'], ['46', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'tropics', 'or', 'sub', 'tropics'], ['20', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'temperate', 'zones'], ['24', 'of', 'threes', 'in', 'the', 'world', 'are', 'in', 'the', 'coniferous', 'boreal', 'forests'], ['there', 'about', '15', 'billion', 'trees', 'are', 'cut', 'down', 'annually'], ['there', 'about', '5', 'billion', 'trees', 'are', 'planted', 'annually']]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the tokens using Keras\n",
    "ks_tokens = []\n",
    "for text in corpus: \n",
    "    ks_tokens.append(keras_text_to_word_sequence(text))\n",
    "print(ks_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'trees',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'according',\n",
       "  'to',\n",
       "  'a',\n",
       "  '2015',\n",
       "  'estimate',\n",
       "  'is',\n",
       "  '3',\n",
       "  '04',\n",
       "  'trillion'],\n",
       " ['46',\n",
       "  'of',\n",
       "  'the',\n",
       "  'trees',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'tropics',\n",
       "  'or',\n",
       "  'sub',\n",
       "  'tropics'],\n",
       " ['20',\n",
       "  'of',\n",
       "  'the',\n",
       "  'trees',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'temperate',\n",
       "  'zones'],\n",
       " ['24',\n",
       "  'of',\n",
       "  'threes',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'coniferous',\n",
       "  'boreal',\n",
       "  'forests'],\n",
       " ['there',\n",
       "  'about',\n",
       "  '15',\n",
       "  'billion',\n",
       "  'trees',\n",
       "  'are',\n",
       "  'cut',\n",
       "  'down',\n",
       "  'annually'],\n",
       " ['there', 'about', '5', 'billion', 'trees', 'are', 'planted', 'annually']]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'number', 'of', 'trees', 'in', 'the', 'world', ',', 'according', 'to', 'a', '2015', 'estimate', ',', 'is', '3.04', 'trillion', '.'], ['46', '%', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'tropics', 'or', 'sub-tropics', '.'], ['20', '%', 'of', 'the', 'trees', 'in', 'the', 'world', 'are', 'in', 'the', 'temperate', 'zones', '.'], ['24', '%', 'of', 'threes', 'in', 'the', 'world', 'are', 'in', 'the', 'coniferous', 'boreal', 'forests', '.'], ['There', 'about', '15', 'billion', 'trees', 'are', 'cut', 'down', 'annually', '.'], ['There', 'about', '5', 'billion', 'trees', 'are', 'planted', 'annually', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the tokens using NLTK\n",
    "# We need to serialize the text to splits the works into tokens\n",
    "nltk_tokens = [] \n",
    "for text in corpus:\n",
    "    pre_text = pd.Series(text).str.cat(sep=' ')\n",
    "    nltk_tokens.append(word_tokenize(pre_text))\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 2. Remove the stop words. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'other', 'of', 'him', 'didn', 'have', 'haven', 'or', 'won', 'couldn']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriving the stop words from the library \n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Examples of some stop words\n",
    "list(stop_words)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['number',\n",
       "  'trees',\n",
       "  'world',\n",
       "  'according',\n",
       "  '2015',\n",
       "  'estimate',\n",
       "  '3',\n",
       "  '04',\n",
       "  'trillion'],\n",
       " ['46', 'trees', 'world', 'tropics', 'sub', 'tropics'],\n",
       " ['20', 'trees', 'world', 'temperate', 'zones'],\n",
       " ['24', 'threes', 'world', 'coniferous', 'boreal', 'forests'],\n",
       " ['15', 'billion', 'trees', 'cut', 'annually'],\n",
       " ['5', 'billion', 'trees', 'planted', 'annually']]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(ks_tokens)): \n",
    "    ks_tokens[i] = [w for w in pd.Series(ks_tokens[i]) if not w in stop_words]\n",
    "ks_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['number',\n",
       "  'trees',\n",
       "  'world',\n",
       "  ',',\n",
       "  'according',\n",
       "  '2015',\n",
       "  'estimate',\n",
       "  ',',\n",
       "  '3.04',\n",
       "  'trillion',\n",
       "  '.'],\n",
       " ['46', '%', 'trees', 'world', 'tropics', 'sub-tropics', '.'],\n",
       " ['20', '%', 'trees', 'world', 'temperate', 'zones', '.'],\n",
       " ['24', '%', 'threes', 'world', 'coniferous', 'boreal', 'forests', '.'],\n",
       " ['15', 'billion', 'trees', 'cut', 'annually', '.'],\n",
       " ['5', 'billion', 'trees', 'planted', 'annually', '.']]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlike the list generated by Keras libary, here we have to make sure all tokens are either capitalized or in lower case.\n",
    "for i in range(0, len(nltk_tokens)): \n",
    "    nltk_tokens[i] = [tok.lower() for tok in nltk_tokens[i]]\n",
    "    nltk_tokens[i] = [w for w in nltk_tokens[i] if not w in stop_words]\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 3. Stemming the tokens, bringing each word to its root. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# We will use Porter algorithm to reduce the words. Porter has 5 phases of word reductions: \n",
    "porter = PorterStemmer()\n",
    "print(porter.stem('Eating'))\n",
    "print(porter.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['number', 'tree', 'world', 'accord', '2015', 'estim', '3', '04', 'trillion'], ['46', 'tree', 'world', 'tropic', 'sub', 'tropic'], ['20', 'tree', 'world', 'temper', 'zone'], ['24', 'three', 'world', 'conifer', 'boreal', 'forest'], ['15', 'billion', 'tree', 'cut', 'annual'], ['5', 'billion', 'tree', 'plant', 'annual']]\n"
     ]
    }
   ],
   "source": [
    "for tokens in ks_tokens:\n",
    "    for i in range(0, len(tokens)):    \n",
    "        tokens[i] = porter.stem(tokens[i])\n",
    "print(ks_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['number', 'tree', 'world', ',', 'accord', '2015', 'estim', ',', '3.04', 'trillion', '.'], ['46', '%', 'tree', 'world', 'tropic', 'sub-trop', '.'], ['20', '%', 'tree', 'world', 'temper', 'zone', '.'], ['24', '%', 'three', 'world', 'conifer', 'boreal', 'forest', '.'], ['15', 'billion', 'tree', 'cut', 'annual', '.'], ['5', 'billion', 'tree', 'plant', 'annual', '.']]\n"
     ]
    }
   ],
   "source": [
    "for tokens in nltk_tokens:\n",
    "    for i in range (0,len(tokens)):    \n",
    "        tokens[i] = porter.stem(tokens[i])\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 4. Remove punctuations, special characters, and words which have length equal to 1. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['number', 'tree', 'world', 'accord', 'estim', 'trillion'], ['tree', 'world', 'tropic', 'sub', 'tropic'], ['tree', 'world', 'temper', 'zone'], ['three', 'world', 'conifer', 'boreal', 'forest'], ['billion', 'tree', 'cut', 'annual'], ['billion', 'tree', 'plant', 'annual']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(ks_tokens)):\n",
    "    new_ks_tokens = []\n",
    "    for tok in ks_tokens[i]: \n",
    "        tok = tok.translate(str.maketrans('', '', string.punctuation))\n",
    "        if tok != \"\" and len(tok) > 1 and (tok.isnumeric()==False):\n",
    "            new_ks_tokens.append(tok)\n",
    "    ks_tokens[i] = new_ks_tokens\n",
    "print(ks_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number', 'tree', 'world', 'accord', 'estim', 'trillion']\n",
      "['tree', 'world', 'tropic', 'sub', 'tropic']\n",
      "['tree', 'world', 'temper', 'zone']\n",
      "['three', 'world', 'conifer', 'boreal', 'forest']\n",
      "['billion', 'tree', 'cut', 'annual']\n",
      "['billion', 'tree', 'plant', 'annual']\n"
     ]
    }
   ],
   "source": [
    "for sentences in ks_tokens: \n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The previous steps has many nexted loops and can easily be very expensive to compute if the number of setences in our corpus grow significantly. Hence, we will repeat the previous steps with few lines of code using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to get an instance of the tokenizer class\n",
    "tok = Tokenizer()\n",
    "# Keras can fit our corpus in a single call\n",
    "tok.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency of each word in the corpus:\n",
      "OrderedDict([('the', 10), ('number', 1), ('of', 4), ('trees', 5), ('in', 7), ('world', 4), ('according', 1), ('to', 1), ('a', 1), ('2015', 1), ('estimate', 1), ('is', 1), ('3', 1), ('04', 1), ('trillion', 1), ('46', 1), ('are', 5), ('tropics', 2), ('or', 1), ('sub', 1), ('20', 1), ('temperate', 1), ('zones', 1), ('24', 1), ('threes', 1), ('coniferous', 1), ('boreal', 1), ('forests', 1), ('there', 2), ('about', 2), ('15', 1), ('billion', 2), ('cut', 1), ('down', 1), ('annually', 2), ('5', 1), ('planted', 1)])\n"
     ]
    }
   ],
   "source": [
    "# Let's explore what is the content of the tokenized corpus\n",
    "print('The frequency of each word in the corpus:')\n",
    "print(tok.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences in the corpus:\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print('The number of sentences in the corpus:')\n",
    "print(tok.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The number of trees in the world, according to a 2015 estimate, is 3.04 trillion.',\n",
       " '46% of the trees in the world are in the tropics or sub-tropics.',\n",
       " '20% of the trees in the world are in the temperate zones.',\n",
       " '24% of threes in the world are in the coniferous boreal forests.',\n",
       " 'There about 15 billion trees are cut down annually.',\n",
       " 'There about 5 billion trees are planted annually.']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of each extracted tokens:\n",
      "{'the': 1, 'in': 2, 'trees': 3, 'are': 4, 'of': 5, 'world': 6, 'tropics': 7, 'there': 8, 'about': 9, 'billion': 10, 'annually': 11, 'number': 12, 'according': 13, 'to': 14, 'a': 15, '2015': 16, 'estimate': 17, 'is': 18, '3': 19, '04': 20, 'trillion': 21, '46': 22, 'or': 23, 'sub': 24, '20': 25, 'temperate': 26, 'zones': 27, '24': 28, 'threes': 29, 'coniferous': 30, 'boreal': 31, 'forests': 32, '15': 33, 'cut': 34, 'down': 35, '5': 36, 'planted': 37}\n"
     ]
    }
   ],
   "source": [
    "print('The index of each extracted tokens:')\n",
    "print(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The appearnce of each words in sentences:\n",
      "defaultdict(<class 'int'>, {'3': 1, 'estimate': 1, 'the': 4, 'of': 4, 'according': 1, 'a': 1, 'trillion': 1, 'world': 4, 'in': 4, 'to': 1, 'trees': 5, 'is': 1, 'number': 1, '2015': 1, '04': 1, '46': 1, 'or': 1, 'are': 5, 'sub': 1, 'tropics': 1, 'temperate': 1, '20': 1, 'zones': 1, 'coniferous': 1, 'boreal': 1, '24': 1, 'threes': 1, 'forests': 1, '15': 1, 'annually': 2, 'cut': 1, 'billion': 2, 'about': 2, 'down': 1, 'there': 2, 'planted': 1, '5': 1})\n"
     ]
    }
   ],
   "source": [
    "print('The appearnce of each words in sentences:')\n",
    "print(tok.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'in': 2,\n",
       " 'trees': 3,\n",
       " 'are': 4,\n",
       " 'of': 5,\n",
       " 'world': 6,\n",
       " 'tropics': 7,\n",
       " 'there': 8,\n",
       " 'about': 9,\n",
       " 'billion': 10,\n",
       " 'annually': 11,\n",
       " 'number': 12,\n",
       " 'according': 13,\n",
       " 'to': 14,\n",
       " 'a': 15,\n",
       " '2015': 16,\n",
       " 'estimate': 17,\n",
       " 'is': 18,\n",
       " '3': 19,\n",
       " '04': 20,\n",
       " 'trillion': 21,\n",
       " '46': 22,\n",
       " 'or': 23,\n",
       " 'sub': 24,\n",
       " '20': 25,\n",
       " 'temperate': 26,\n",
       " 'zones': 27,\n",
       " '24': 28,\n",
       " 'threes': 29,\n",
       " 'coniferous': 30,\n",
       " 'boreal': 31,\n",
       " 'forests': 32,\n",
       " '15': 33,\n",
       " 'cut': 34,\n",
       " 'down': 35,\n",
       " '5': 36,\n",
       " 'planted': 37}"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> Approach #1: One Hot Encoding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tok.texts_to_matrix(corpus, mode='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 3. 2. 1. 1. 1. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 3. 2. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 2. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the frequency of each word in the sentence (TFIDF)\n",
    "print(tok.texts_to_matrix(corpus, mode='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.125      0.0625     0.0625     0.         0.0625\n",
      "  0.0625     0.         0.         0.         0.         0.\n",
      "  0.0625     0.0625     0.0625     0.0625     0.0625     0.0625\n",
      "  0.0625     0.0625     0.0625     0.0625     0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.21428571 0.14285714 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.14285714 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07142857 0.07142857\n",
      "  0.07142857 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.25       0.16666667 0.08333333 0.08333333 0.08333333\n",
      "  0.08333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08333333 0.08333333 0.08333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.16666667 0.16666667 0.         0.08333333 0.08333333\n",
      "  0.08333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08333333 0.08333333\n",
      "  0.08333333 0.08333333 0.08333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.11111111 0.11111111 0.\n",
      "  0.         0.         0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.125      0.125      0.\n",
      "  0.         0.         0.125      0.125      0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.125      0.125     ]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the frequency of each word in the sentence\n",
    "print(tok.texts_to_matrix(corpus, mode='freq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Building a word2vec model typically is very expensive, so we need to keep in mind the multiprocessing \n",
    "# approach option\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# size (int, optional) – Dimensionality of the word vectors.\n",
    "# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "model = gensim.models.Word2Vec (ks_tokens, window=10, size=150, min_count=1,workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': <gensim.models.keyedvectors.Vocab at 0x7ff806721978>,\n",
       " 'tree': <gensim.models.keyedvectors.Vocab at 0x7ff8067211d0>,\n",
       " 'world': <gensim.models.keyedvectors.Vocab at 0x7ff8067213c8>,\n",
       " 'accord': <gensim.models.keyedvectors.Vocab at 0x7ff806721be0>,\n",
       " 'estim': <gensim.models.keyedvectors.Vocab at 0x7ff806721ba8>,\n",
       " 'trillion': <gensim.models.keyedvectors.Vocab at 0x7ff806721c50>,\n",
       " 'tropic': <gensim.models.keyedvectors.Vocab at 0x7ff806721748>,\n",
       " 'sub': <gensim.models.keyedvectors.Vocab at 0x7ff8067216d8>,\n",
       " 'temper': <gensim.models.keyedvectors.Vocab at 0x7ff8067219b0>,\n",
       " 'zone': <gensim.models.keyedvectors.Vocab at 0x7ff8067219e8>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x7ff806721cc0>,\n",
       " 'conifer': <gensim.models.keyedvectors.Vocab at 0x7ff806721cf8>,\n",
       " 'boreal': <gensim.models.keyedvectors.Vocab at 0x7ff806721d30>,\n",
       " 'forest': <gensim.models.keyedvectors.Vocab at 0x7ff806721d68>,\n",
       " 'billion': <gensim.models.keyedvectors.Vocab at 0x7ff806721da0>,\n",
       " 'cut': <gensim.models.keyedvectors.Vocab at 0x7ff806721dd8>,\n",
       " 'annual': <gensim.models.keyedvectors.Vocab at 0x7ff806721e10>,\n",
       " 'plant': <gensim.models.keyedvectors.Vocab at 0x7ff806721e48>}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.36090941e-04 -6.70538648e-05 -1.57671515e-03 -2.40869750e-03\n",
      "  3.27374879e-03 -1.06257598e-04  9.91373206e-04 -5.49071992e-04\n",
      " -5.52719168e-04 -1.43252939e-04 -7.86744422e-05  3.62405641e-04\n",
      "  1.18917448e-03  2.43008789e-03 -1.28880306e-03  2.64223985e-04\n",
      "  9.29775299e-04  4.96292487e-04  4.87394136e-04  1.58919895e-03\n",
      " -4.16911033e-04  1.97249488e-03  1.21223717e-03  1.09736237e-03\n",
      "  8.09291203e-04  6.99476688e-04  2.76509370e-03 -2.60518282e-03\n",
      " -3.28214467e-03  8.07532982e-04  1.03925646e-03  1.08913810e-04\n",
      "  1.91287731e-03  2.76876660e-03  1.63322047e-03  2.24159122e-03\n",
      " -8.30278805e-05  1.57238683e-04  1.27926655e-03 -1.42196019e-03\n",
      "  2.78833043e-03 -1.41919788e-03  1.04285835e-03  2.52929749e-03\n",
      " -1.31320980e-04 -5.75431477e-05 -8.74717953e-04 -3.08527681e-03\n",
      " -2.05976285e-05  2.31822627e-03 -2.88447062e-03 -1.04455205e-04\n",
      "  6.27650123e-04 -1.87849556e-03  2.01445189e-03  2.56019924e-03\n",
      "  9.62449820e-04 -3.22310789e-03 -8.84222507e-04  2.13926265e-04\n",
      "  2.20695933e-04  3.46532703e-04 -2.73278391e-04  2.25163042e-03\n",
      "  2.97942525e-03 -2.01902213e-03  1.86705182e-03 -2.92427163e-03\n",
      "  2.99562886e-03 -1.69835356e-03  2.35085282e-03 -5.95642196e-04\n",
      "  9.47297551e-04 -1.35863505e-04 -2.34322972e-03 -2.37070117e-03\n",
      "  8.61711043e-04 -1.08437042e-03 -9.95271606e-04  1.30905607e-03\n",
      "  2.95994012e-03 -1.18390354e-03  2.24528182e-03  2.54242950e-05\n",
      " -2.03394657e-03  2.66104634e-03  2.97279702e-03  2.61345133e-03\n",
      " -3.06339329e-03  1.16239826e-03  3.92305286e-04  1.67238724e-03\n",
      "  2.52688699e-03  5.06284938e-04  1.16401853e-03 -1.43377704e-03\n",
      " -1.82696071e-03  3.03890789e-04  1.17643888e-03  9.13544733e-04\n",
      "  1.11458544e-03  2.36433395e-03 -1.45488812e-04  1.19383354e-03\n",
      "  2.93722213e-03 -2.89577479e-03  1.39666360e-03  4.80875548e-04\n",
      " -3.13670980e-03  1.21444534e-03 -9.35565331e-04  2.32036365e-03\n",
      " -3.19432002e-03 -9.42467363e-04  1.91230734e-04  3.16606974e-03\n",
      " -2.44488707e-03  8.22269125e-04 -1.76322751e-03 -1.55346366e-04\n",
      " -2.22186395e-03 -1.74124155e-03 -2.94979173e-03  2.06196774e-03\n",
      " -1.70824211e-03  1.25378615e-03 -2.94799334e-04 -2.77051260e-03\n",
      " -1.45609514e-03  1.65360223e-03  2.41713040e-03  1.18579390e-03\n",
      " -1.87778031e-04 -1.83666195e-03  2.72052269e-03  9.72653506e-04\n",
      " -3.07441829e-03  5.60728775e-04  1.15640229e-03 -6.79627468e-04\n",
      " -1.77323946e-03  8.61362205e-04  6.91511668e-05  1.83290313e-03\n",
      " -2.30975728e-03  1.37953949e-03  3.06457211e-03  1.10625778e-03\n",
      "  2.46183103e-04  1.68358325e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedalhamid/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
